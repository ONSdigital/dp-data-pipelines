# Data transit requirements

[DIS706 Jira ticket](https://jira.ons.gov.uk/browse/DIS-706)

## Introduction

This document outlines the requirements for inputs to the Data Transformation and Validation (DTV) pipeline. The recommendations below are predicated on an assumption that input files are received into an S3 bucket as a single `.tar` file. The DTV pipeline decompresses the `.tar` file and transforms the input files into a number of output files which should be uploaded to the DP Upload Service `/upload-new` endpoint. Output files are uploaded to an S3 bucket that has already been specified - for example, in the Sandbox environment, this is the "ons-dp-sandbox-encrypted-datasets" bucket. Files larger than 5MB are chunked, and these chunks are uploaded separately and reassembled in the "datasets" folder of the pre-defined bucket.

As it is currently designed, the DTV pipeline allows users to submit multiple files, compressed into one `.tar` file:

- 1-n input files to be transformed.
- 0-n supplementary distributions (e.g. alternative data formats) that do not need to be transformed but form part of the submission.

Within the DTV pipeline code, the existence of these files is confirmed using a pipeline configuration dictionary, e.g:

```python
pipeline_config = {
    "config_version": 1,
    "transform": sdmx_compact_2_0_prototype_1,
    "transform_inputs": {
        "^data1.xml$": sdmx_sanity_check_v1,
        "^data2.xlsx$": sdmx_sanity_check_v1
    },
    "transform_kwargs": {},
    "required_files": [
        {"matches": "^data1.xml$", "count": "1"},
        {"matches": "^data2.xlsx$", "count": "1"}
    ],
    "supplementary_distributions": [
        {"matches": "^data3.ods$", "count": "1"}
    ],
    "secondary_function": dataset_ingress_v1,
}
```

In this example, there are **two** input files to be transformed (`required_files`: `data1.xml` and `data2.xlsx`), and **one** additional file that does not need transforming, but forms part of the submission (`supplementary_distributions`: `data3.ods`). The pipeline will fail if any of these files are not present in the `.tar` file.

Once the transformation process has finished, the following output files will have been generated, ready to be uploaded to the relevant API:

- `data1.csv` and `data2.csv` (CSV files of `data1.xml` and `data2.xlsx` following transformation). Uploaded to DP Upload Service API.
- `data1.json` and `data2.json` (JSON files of catalog metadata associated with `data1.xml` and `data2.xlsx`). Submitted to DP Dataset API as a `POST` request.
- `data3.ods` (supplementary distribution). Uploaded to DP Upload Service API.

In order for these files to be uploaded to the relevant API, certain HTTP request parameters need to be specified. We are therefore recommending that a `manifest.json` file **must** be included in the `.tar` file, containing the details required to populate these parameters.

## Data Transformation and Validation Pipeline requirements

Accept a `.tar` file via an S3 bucket containing the following files:
- `manifest.json` file containing the metadata required to process and upload the input file.
- 1-n input files to be transformed.
- 0-n supplementary distributions that do not need to be transformed but form part of the submission.

Upload the following files to the relevant API:
- `.csv` output file of data from each input file generated by the pipeline transform (to DP Upload Service API).
- `.json` file of catalog metadata associated with each `.csv` file generated by the pipeline transform (to DP Dataset API).
- 0-n supplementary distributions contained within the original `.tar` submission (to DP Upload Service API).

All of the `.csv` output files and supplementary distribution files should be uploaded to the same location, as these form a complete submission. 

## Challenges

As stated in the [Introduction](#introduction) section, output files are uploaded to a pre-defined S3 bucket. The `path` field listed in the [Transport Pipeline #3](#required-fields-for-transport-pipeline-3) table below is defined as "The file path that the file should be available at once published to web and api users". Currently, when generating the HTTP request parameters, the DTV pipeline is generating a separate `path` value for each of the `.csv` output files and supplementary distribution files. For the example above, if using the Sandbox environment, the generated `path` and published file location would be as follows:

| File         | `path`                            | File location                                                                       |
|--------------|-----------------------------------|-------------------------------------------------------------------------------------|
| `data1.csv`  | `datasets/<timestamp>-data1-csv`  | `s3://ons-dp-sandbox-encrypted-datasets/datasets/<timestamp>-data1-csv/data1.csv`   |
| `data2.xlsx` | `datasets/<timestamp>-data2-xlsx` | `s3://ons-dp-sandbox-encrypted-datasets/datasets/<timestamp>-data2-xlsx/data2.xlsx` |
| `data3.ods`  | `datasets/<timestamp>-data3-ods`  | `s3://ons-dp-sandbox-encrypted-datasets/datasets/<timestamp>-data3-ods/data3.ods`   |

This would not be correct, as these three files should all be contained in the same folder within the bucket. One of the fields that we are recommending for inclusion in `manifest.json` is `dataset_id`, which would permit the following folder structure to be implemented:

| File         | `path`                               | File location                                                                       |
|--------------|--------------------------------------|-------------------------------------------------------------------------------------|
| `data1.csv`  | `datasets/<dataset_id>/<timestamp>/` | `s3://ons-dp-sandbox-encrypted-datasets/datasets/dataset_id/<timestamp>/data1.csv`  |
| `data2.xlsx` | `datasets/<dataset_id>/<timestamp>/` | `s3://ons-dp-sandbox-encrypted-datasets/datasets/dataset_id/<timestamp>/data2.xlsx` |
| `data3.ods`  | `datasets/<dataset_id>/<timestamp>/` | `s3://ons-dp-sandbox-encrypted-datasets/datasets/dataset_id/<timestamp>/data3.ods`  |

This proposal would require the ability for users to submit multiple files as a single `.tar` file via, for example, Sharepoint. Assurance is sought from the CloudOps team that this is feasible. Some form of validation that all required files are present in the `.tar` file prior to submission to DTV will also need to be implemented.

## Supplementary information

The recommendations in this document are based on the AND Solution Design for [Transfer of input files](https://confluence.ons.gov.uk/display/DIS/Transfer+of+input+files), in particular [Transform pipeline #3](https://confluence.ons.gov.uk/display/DIS/Transfer+of+input+files#Transferofinputfiles-Transportpipeline#3).

The DP Upload Service is written in Go. There are (at least) two locations in the DP codebase where metadata needs to be provided in a precise format in order for HTTP requests to succeed - these are linked to below:

- [FileMetadata struct](https://github.com/ONSdigital/dp-api-clients-go/blob/main/files/data.go#L3)
- [Resumable struct](https://github.com/ONSdigital/dp-upload-service/blob/develop/upload/upload.go#L19)

### Required fields for Transport Pipeline #3

The table below is copied from the [Transform pipeline #3](https://confluence.ons.gov.uk/display/DIS/Transfer+of+input+files#Transferofinputfiles-Transportpipeline#3) section. There is significant overlap between the fields in this table and the fields in the `FileMetadata` and `Resumable` structs linked to in the [Supplementary information](#supplementary-information) section.

| Field              | Required? | Description                                                                                                                      |
|--------------------|-----------|----------------------------------------------------------------------------------------------------------------------------------|
| fileAuthorEmail    | Mandatory | Email address of the file author. Used for notifications generated during data transformation and validation pipeline processing |
| fileAuthorUsername | Mandatory | Username of the file author. Used for notifications generated during data transformation and validation pipeline processing      |
| fileName           | Mandatory | Name of SDMX V2.0 to be transported to the data transformation and validation pipeline, and onwards to the static file system    |
| fileVersion        | Mandatory | Version number of the file. There may be a requirement to upload updated files of the same name                                  |
| isPublishable      | Mandatory | Whether the file is intended to by published to web and api users by the static file system                                      |
| licence            | Mandatory | The licence that applies to the file once published to web and api users                                                         |
| licenceUrl         | Mandatory | The url where the licence described by the licence field is located                                                              |
| manifestVersion    | Mandatory | A version number to allow the support different manifest fields in the future, if required                                       |
| path               | Mandatory | The file path that the file should be available at once published to web and api users                                           |
| aliasName          | Optional  | Alias for the file to be uploaded to the static file system                                                                      |
| collectionId       | Optional  | Id of an existing collection in Florence that the file should be attached to once it is uploaded to the static file system       |

## Recommendations for `manifest.json` structure

Having reviewed the requirements for Transport Pipeline #3, the following fields should be included in the `manifest.json` file:

```json
{
    "manifestVersion": "integer",
    "dataset_id": "string",
    "fileAuthorEmail": "string",
    "fileAuthorUsername": "string",
    "isPublishable": "Optional boolean",
    "licence": "Optional string",
    "licenceUrl": "Optional string",
    "title": "Optional string",
    "aliasName": "Optional string"
}
```

Some of these fields are required only for interacting with the DP Upload Service (via `UploadServiceClient._get_upload_new_params()`), and some are used to configure the pipeline itself. Further information on each field is in the table below.

### `manifest.json` fields

| Field                | Required? | Used in pipeline?                                                                                             | Default value                                                               | Go struct            |
|----------------------|-----------|---------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------|----------------------|
| `manifestVersion`    | Mandatory | No, would be used to validate the `manifest.json` file against a schema to ensure required fields are present | None                                                                        | N/A, pipeline config |
| `dataset_id`         | Mandatory | Yes, to get transform details from `CONFIGURATION` in `dpypelines.pipeline.configuration.py`                  | None                                                                        | N/A, pipeline config |
| `fileAuthorEmail`    | Mandatory | No, but could replace env var in `dpypelines.pipeline.shared.utils.get_submitter_email()`                     | None                                                                        | Unknown              |
| `fileAuthorUsername` | Mandatory | No, but could be used to personalise emails sent by `SesClient`                                               | None                                                                        | Unknown              |
| `isPublishable`      | Optional  | Yes, used in `UploadServiceClient._get_upload_new_params()`                                                   | False                                                                       | `FileMetadata`       |
| `licence`            | Optional  | Yes, used in `UploadServiceClient._get_upload_new_params()`                                                   | "Open Government Licence v3.0"                                              | `FileMetadata`       |
| `licenceUrl`         | Optional  | Yes, used in `UploadServiceClient._get_upload_new_params()`                                                   | "http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/" | `FileMetadata`       |
| `title`              | Optional  | Yes, used in `UploadServiceClient._get_upload_new_params()`                                                   | Filename without extension                                                  | `FileMetadata`       |
| `aliasName`          | Optional  | Yes, used in `UploadServiceClient._get_upload_new_params()`                                                   | Filename with extension                                                     | `Resumable`          |

The following [Transport Pipeline #3](#required-fields-for-transport-pipeline-3) fields have been omitted for the stated reasons:

| Field        | Omission reason                                                  |
|--------------|------------------------------------------------------------------|
| fileName     | Can be inferred from the name of the file provided               |
| fileVersion  | Outside of DTV scope                                             |
| path         | Can be inferred from combination of bucket name and `dataset_id` |
| collectionId | Outside of DTV scope                                             |
